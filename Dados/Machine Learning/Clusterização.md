
**Resumo**

Organizar. Separar. Segmentar. Agrupar. Quando lidamos com desafios complexos no nosso dia a dia, faz parte do processo tentar encontrar padrões para separar e dividir esse problema em partes menores que podem ser organizadas como etapas, passos ou partes de um todo. Isso ocorre porque a resolução completa e simultânea demandaria muito mais esforço e, em alguns casos, talvez se tornasse impossível.

Encontrar padrões é algo que a mente humana faz de forma quase automática. Nesse contexto, é relevante considerar a forma como organizamos, por exemplo, grupos nos aplicativos de mensagem. Temos nossa lista de contatos, mas, em geral, somos mais próximos de algumas pessoas, que acabam ficando em uma lista de favoritos ou amigos próximos. Criamos grupos para nos comunicarmos melhor e temos um lugar específico para conversar com nossos amigos do colégio, com o pessoal da faculdade, os colegas do trabalho e, claro, um grupo da família.

Agrupar é algo que nós, seres humanos, fazemos de maneira intuitiva. Naturalmente, organizamos o mundo que nos cerca por meio de grupos e simplificações. Um exemplo clássico é pensar na forma como lidamos com música em nosso cotidiano. Em geral, as dividimos e categorizamos de várias formas. Podemos agrupar músicas por ano de lançamento, país de origem, artista, gênero ou mesmo uma combinação dessas características, conforme fizer sentido para nós.

Outro exemplo de como aplicamos o agrupamento em nossa vida é a maneira como resolvemos um quebra-cabeça. De partida, todas as peças estão misturadas, e começar a montagem diretamente nesse momento pode dar muita dor de cabeça. A estratégia é separar, ou agrupar, as peças da borda. Num segundo momento, podemos agrupar, ou segmentar, as peças por cor ou tonalidade, colocando peças parecidas entre si em pilhas separadas e deixando as peças muito diferentes em outras pilhas, onde elas serão semelhantes às que estão lá. A ideia é que, uma vez que as peças estão separadas, deixaremos de lidar com um problema de 500 peças e passaremos a trabalhar com alguns grupos homogêneos e bem menores.

Essa capacidade de gerar grupos, que aparecem em nossa vivência diária, pode ser transportada também para o mundo corporativo. Desafios empresariais podem ser abordados e resolvidos através desse tipo de pensamento. Pode-se, por exemplo, agrupar nossos clientes por idade, para assim fazer alguma ação de marketing segmentada. Ou podemos separar nossos fornecedores por tipo de material que entregam ou por quão distante geograficamente estão de nós. Podemos tentar mapear as habilidades dos nossos colaboradores para identificar grupos de habilidades e entender melhor os pontos fortes e fracos do nosso time.

Perceba que, quando estamos lidando com uma, duas ou até mesmo três características ou atributos daquilo que estamos agrupando, conseguimos trabalhar de forma bastante eficiente. Se o número de observações que tivermos disponíveis for relativamente pequeno, digamos, uma base de dados com cem ou duzentos fornecedores, por exemplo, conseguimos analisar manualmente e, com base em nossa experiência e nos dados, criar grupos de forma intuitiva que podem ser valiosos para a operação diária da organização.

Porém, quando o volume de dados aumenta, esse processo se torna muito mais desafiador. Imagine analisar uma base de dados com transações diárias de cinquenta mil clientes ao longo do último ano para identificar cinco ou seis segmentos que serão utilizados para definir a estratégia de divulgação da empresa no próximo semestre. Além da quantidade de linhas ou observações disponíveis, também surge como desafio as características ou atributos que compõem cada linha da nossa base de dados. Provavelmente, teremos informações de data e hora da transação, valor, canal utilizado e ID do cliente. Talvez tenhamos algumas características do cliente que devem fazer parte da análise e características da próxima transação. Ela é positiva ou negativa? Trata-se da aquisição de um produto ou serviço? Quanto tempo ela durou? É uma transação que todos os clientes realizam com frequência ou é algo de relativa raridade? Todas essas perguntas podem ser mapeadas para nossa base de dados como colunas, atributos ou características que devem fazer alguma diferença na definição de um segmento.

Como lidar com a tal base de dados? É aqui que chegamos nos Algoritmos de Clusterização. Esses algoritmos surgem de um ramo da Ciência da Computação chamado Inteligência Artificial (IA). A proposta desse campo de pesquisa é desenvolver sistemas ou computadores capazes de imitar ou reproduzir o comportamento e o pensamento humano.

Para criar tais sistemas inteligentes, uma das estratégias que podem ser empregadas é o que chamamos de Aprendizado de Máquina, em inglês, Machine Learning. Também chamado de Aprendizagem Automática, refere-se à utilização de algoritmos que, a partir da análise e exploração de uma base de dados, são capazes de identificar padrões e criar regras que expandem e extrapolam a capacidade humana. Com esses algoritmos, somos capazes, por exemplo, de criar sistemas ou modelos capazes de identificar transações fraudulentas, prever  e prevenir panes de sistemas, identificar doenças ou mesmo dirigir veículos de forma autônoma. O Agrupamento é uma vertente de Aprendizado de Máquina.

Clusterização (Clustering/Agrupamento) é o nome dado a um conjunto de técnicas que combinam estatística, matemática e computação para segmentar objetos, criar categorias e entender relações. Esse processo é realizado a partir da análise de similaridades e diferenças existentes que caracterizam os dados. A aplicação dessas técnicas pode ser feita em uma vasta gama de contextos e tratar informações de clientes, fornecedores, pessoas, personagens, animais ou qualquer dado que possa ser representado de forma tabular (estruturada), bem como lidar com dados não estruturados, tais como imagens e texto.

A tarefa de Agrupamento faz parte de um ramo do Aprendizado de Máquina conhecido como Aprendizagem Não Supervisionada. O que caracteriza esse ramo de Aprendizagem é que os dados que serão analisados não possuem uma classe, que pode também ser chamada de variável resposta ou variável dependente. Os métodos utilizados nesse contexto buscam encontrar padrões nos dados e utilizam medidas de similaridade e dissimilaridade para agrupar e extrair valor dos dados sem que haja conhecimento a priori sobre ele.

Importante lembrar que, embora nos dados não haja indicação da resposta correta a ser encontrada e o algoritmo de aprendizagem não tenha uma meta claramente definida, um ponto fundamental para o sucesso de iniciativas que utilizam métodos de Clusterização é garantir a participação de especialistas no domínio ou negócio, que irão acompanhar a seleção inicial das características e dados que serão utilizados, ou variáveis, auxiliar na representação do problema e, ao final, validar os possíveis grupos encontrados pelo modelo.

A utilização de algoritmos de Agrupamento apresenta alguns desafios intrínsecos. Tais desafios devem ser endereçados tanto de um ponto de vista qualitativo, através de uma análise cuidadosa e criteriosa dos dados, quanto de forma quantitativa, fazendo uso de métodos e estratégias que visam mitigar os efeitos desses problemas. Alguns desses desafios são:

- **Resposta correta**: quando iniciamos o processo para utilizar um algoritmo de Clusterização, de forma geral, isso significa que não temos uma variável resposta para nos guiar. Os padrões que existem nos dados irão emergir, por isso é necessário que estejamos atentos para lidar com cada um deles de forma adequada e crítica.
- **Número de Grupos a serem encontrados**: não existe, a priori, uma indicação de quantos grupos existem nos dados. Deve-se sempre considerar o que especialistas do domínio trazem, ao mesmo tempo em que se utilizam técnicas matemáticas e uma pitada de bom senso para definir o número ideal, ou ao menos o mais adequado, de clusters.
- **Similaridade**: definir de forma quantitativa o que é ser similar ou dissimilar, pois é uma tarefa desafiadora. Seres humanos conseguem identificar padrões em duas ou três dimensões com facilidade, muitas vezes de forma intuitiva, sem se preocupar com quais critérios estão sendo utilizados de forma subjetiva. Quando estamos lidando com algoritmos, todas as variáveis precisam estar disponíveis e claras.

Além de Clusterização, existem outras estratégias de Aprendizagem Não Supervisionada, tais como Regras de Associação, sendo um dos algoritmos mais utilizados, e Redução de Dimensionalidade, na qual podemos citar como exemplos: a Análise de Componentes Principais ou PCA (Principal Component Analysis) e o SVD (Singular Value Decomposition).

-----

## Análise de Clusters

Tornando o processo de encontrar grupos em algo **escalável, estruturado, mensurável e reprodutível.**

A análise de clusters é definida como o processo de “Encontrar grupos de objetos de tal forma que em um grupo sejam similares (ou relacionados) entre si e diferentes de (ou não relacionados) a objetos em outros grupos” (Tan et al., 2006). Um ponto importante antes de passarmos pelas etapas da análise é entender alguns processos e metodologias comuns que não se caracterizam como Agrupamento, tais como:

- Classificação
- Segmentação Simples
- Resultado de uma consulta SQL

Podemos dividir a Análise de Clusters em 4 etapas separadas e igualmente importantes: **Representação do Problema, Seleção da Medida de Proximidade, Seleção de um Algoritmo de Agrupamento e Avaliação dos Grupos Encontrados.**


**Representação do Problema**

Começamos pela primeira fase, Representação do Problema. Nessa fase, duas definições importantes precisam acontecer: a primeira é o Entendimento do Problema e a segunda é a Preparação dos Dados.

Primeiramente, é necessário compreender qual é a natureza dos dados e do problema que será tratado. Ser capaz de se comunicar com o cliente ou especialista do domínio é essencial para entender o que se espera que seja modelado e o motivo. Entender em que contexto os dados serão utilizados, como eles são obtidos, o que significa cada um deles, além do que já é feito hoje para lidar com esse desafio.

Uma vez que o Entendimento do Problema esteja claro, o próximo passo é estruturar os dados para que estejam aptos a serem utilizados em um algoritmo de Clusterização. Aqui, o que se espera é conseguir organizar uma base de dados de forma que as informações nela contida representem da melhor forma possível as instâncias que esperamos modelar. Seguem algumas garantias iniciais que se devem levar em consideração:


**Data Tidy – Dados Tidy:**

Dados Tidy definem uma forma padrão de conectar a **estrutura** de uma base de dados, seu formato físico, com sua **semântica**, ou seja, seu significado.

- **Estrutura**: os dados são definidos em forma de tabela, contendo linhas e colunas.
- **Semântica**: o significado das informações que o banco de dados contém. Quando se fala nessa estrutura, os dados podem ser organizados considerando duas visões: Variáveis e Observações.

**As 3 Regras Tidy**

1. Cada Variável é uma coluna.
2. Cada Observação é uma linha.
3. Cada Tabela contém um tipo de dado.

**Missings – Valores Faltantes**

Algo importante a se analisar é se os dados a que temos acesso possuem dados faltantes. Isso porque, em geral, os algoritmos que usamos em Aprendizado de Máquina não são capazes de lidar com esse tipo de dado. Além disso, Variáveis com muitos dados faltantes podem inserir algum tipo de ruído em nosso modelo.


**Outliers**

São valores que destoam do restante dos dados. Por exemplo, em uma base de dados com idade de clientes, alguém com uma idade de 300 anos é um outlier; nesse caso, provavelmente o valor é resultado de um erro de digitação. No entanto, nem sempre somos capazes de determinar de forma tão direta o motivo dos outliers que encontramos em nossos dados.

**Normalização**

Normalmente, Algoritmos de Aprendizado de Máquina necessitam de dados numéricos para funcionar. Isso acontece porque, em geral, fazem uso de funções matemáticas e estatísticas que dependem de entradas numéricas para operar corretamente.

Além da necessidade de trabalhar com valores numéricos, quando falamos de algoritmos de Agrupamento, é necessário também garantir que esses valores tenham escalas parecidas.

  
**Selecionar uma Medida de Proximidade**

Depois de finalizada a parte de Representação do Problema, a segunda fase da Análise de Clusters é escolher uma Medida de Proximidade ou Similaridade.

Clustering é sobre agrupar objetos parecidos ou similares. O quão um objeto é similar a outro depende da distância entre eles. A forma de definir e calcular essa distância é bastante subjetiva e depende do tipo de item que estamos analisando. Sendo assim, é importante termos algum domínio sobre o tipo de dado com o qual estamos lidando.

Para definir a similaridade entre objetos, os algoritmos de clusterização utilizam distâncias. Algumas das mais comuns são:

- **Distância Euclidiana:** Tende a ser a métrica mais utilizada em projetos de Clustering.
- **Distância Manhattan:** Calcula a distância absoluta entre dois pontos.
- **Distância Hamming:** Uma forma de calcular distâncias entre variáveis categóricas.
- **Similaridade por Cosseno:** Calcula a similaridade entre dois vetores em um espaço vetorial. Normalmente, essa métrica é utilizada em mineração de texto, por exemplo, para identificação de tópicos.

**Selecionar o Algoritmo de Agrupamento**

O Algoritmo de Agrupamento é a ferramenta que utilizaremos para encontrar grupos em nossos dados. Uma vez que tenhamos estruturado uma maneira de Representar o Problema e escolhemos uma Medida de Distância, o próximo passo é escolher o Algoritmo que será utilizado. O algoritmo escolhido, bem como a medida de distância, tem grande impacto sobre os grupos que serão formados.

De modo geral, os principais algoritmos de agrupamento podem ser divididos em três grandes grupos:

1. **Hierárquicos:** Organiza os dados em uma estrutura similar a uma árvore em que os dados possam ser entendidos de forma hierárquica, com um nível maior em que todos os demais estão contidos, até um último nível em que cada observação é um grupo.
2. **Particional:** São métodos que buscam encontrar partições em uma base de dados.
3. **Baseados em Densidade:** Utiliza o conceito de densidade para gerar os grupos. Aqui, um grupo é definido como uma região do espaço com uma alta densidade de pontos.

**Avaliação de Grupos**

"A validação das estruturas de grupos encontrados é a parte mais difícil e frustrante da Análise de Clusters. Sem um esforço consciente nessa direção, a Análise de Clusters continuará a ser uma Arte das Trevas, acessível apenas aos escolhidos, que possuem experiência e grande coragem." (Jain e Dubes, Algorithms for Clustering Data, 1988).

Existem diferentes procedimentos para avaliar de maneira objetiva e quantitativa os resultados de análise de clustering. Cada um desses procedimentos pode nos ajudar a responder uma ou mais questões do tipo:

Encontramos grupos de fato?

- Grupos são pouco usuais ou facilmente encontrados ao acaso?
- Qual a qualidade (relativa ou absoluta) dos grupos encontrados?
- Qual é o número natural / mais apropriado de grupos?

Existem três formas de analisar a qualidade dos grupos encontrados. Podem ser utilizados Critérios Internos, Externos e Relativos.


**Tópicos Avançados**

Nossa segunda aula foi sobre Análise de Clusters. Definimos o que é, quais são suas fases e exploramos com alguma profundidade cada uma dessas etapas. Caso tenha interesse em se aprofundar no tema, sugiro que acesse os conteúdos a seguir:

- **Análises com algoritmos de Clustering** - texto escrito por Isnard Gurgel, disponível no Medium.
- **Como usar Clusters para encontrar padrões nos seus dados | ML do Básico ao Aplicado** - vídeo do canal Rafinha dos Dados, disponível no Youtube.
- **Conheça as Etapas do Pré-Processamento de dados** - texto escrito por Pedro César Tebaldi Gomes, disponível no site Data Geeks.
- **Organizando banco de dados: uma introdução ao conceito de Tidy Data** - texto escrito por Dalton Costa, disponível no Medium.
- **9 Distance Measures in Data Science** - texto escrito por Maarten Grootendorst, disponível no site Towards Data Science.

# Clusterização Hierárquica

Clusterização Hierárquica é uma das abordagens utilizadas no contexto de Aprendizado Não-Supervisionado para encontrar grupos a partir da análise dos dados, buscando padrões e similaridades entre as observações disponíveis. Essa estratégia busca criar uma hierarquia de clusters ou grupos, em que os grupos em um determinado nível dessa hierarquia contêm os grupos do nível inferior e estão inseridos no grupo da camada superior.

Para gerar essa estrutura, os Algoritmos Hierárquicos utilizam uma Matriz de Distâncias. Essa matriz contém a distância entre todas as observações ou entre todos os pontos. Com essa informação, os grupos são criados combinando os dois elementos com a menor distância entre eles, ou seja, os que são mais similares. Esse processo é interativo e a cada rodada, um dos objetos mais próximos é combinado.

![[Pasted image 20250522075402.png]]

Existem duas estratégias para implementar um Algoritmo Hierárquico, Abordagem Aglomerativa e Abordagem Divisiva.

  
**Abordagem Aglomerativa**

A estratégia Aglomerativa, em inglês Agglomerative Clustering, é uma forma de resolver problemas de agrupamento em que, de partida, **todas as observações da base de dados são tratadas como um cluster, ou grupo, individual.** Assim, inicialmente, calcula-se a distância de todas as observações umas para as outras, gerando uma Matriz de Distância e, em seguida, os pontos mais próximos vão sendo combinados até que exista apenas um único grupo com todas as observações.

Algoritmo:

1. Iniciar tratando cada observação como um cluster
2. Definir o melhor par de clusters para combinar
3. Combinar o par de clusters escolhido
4. Repetir até que todas as observações estejam em um único cluster

**Abordagem Divisiva**

A estratégia Divisiva, em inglês Divisive Clustering, é uma abordagem para resolver problemas de agrupamento que, de partida, considera que **todas as observações fazem parte de um único grupo**. Em seguida, utilizando algum método para dividir a base de dados, cria-se grupos e subgrupos até que todas as observações tenham formado grupos individuais.

Algoritmo:

1. Iniciar todas as observações em um único cluster
2. Subdividir o cluster em dois novos clusters
3. Em cada um dos novos clusters aplicar essa estratégia e subdividi-los até que cada observação seja um cluster

Quando trabalhamos com a Matriz de Distâncias considerando as observações individuais, é relativamente simples entender como a distância entre duas observações deve ser calculada. No entanto, quando precisamos calcular a distância entre grupos ou entre grupos e observações, essa tarefa pode se tornar um pouco mais desafiadora. Por isso, existem várias estratégias para calcular a Distância Inter-Cluster ou Dissimilaridade Inter-Cluster:

- **Single Linkage:** A distância entre dois grupos é definida pela distância entre os dois membros de cada grupo que estão mais próximos.

- **Complete Linkage:** A distância entre dois grupos é definida pela distância entre os dois membros de cada grupo que estão mais distantes.

- **Average Linkage:** A distância entre dois grupos é definida pela média das distâncias de todos os membros de um grupo para todos os membros do outro grupo.

 **Ward's Linkage:**

- Para cada grupo, é definido calculado o valor central (média).
- Em cada grupo, a distância de todos os membros para o valor central é somada.
- Combina os grupos que, ao serem unidos, geram a menor soma das distâncias intra-cluster.

**Observação:** Linkage pode ser entendido como conexão.

Seja qual for a estratégia utilizada para implementar o algoritmo de Clusterização Hierárquica e a métrica para a distância inter-cluster selecionada, um ponto ainda não explorado é: como encontramos os grupos após esse processo? Lembre-se: ao final, todas as observações são clusters individuais ou todas elas estão em um único cluster. Para definir os grupos, é necessário estabelecer um ponto de corte e, para isso, uma ferramenta de visualização pode ser essencial: o **Dendrograma**.

A estrutura de árvore que emerge ao utilizar os métodos de Agrupamento Hierárquico é chamada de Dendrograma. É um diagrama que nos ajuda a visualizar e compreender as distâncias e relações entre as observações da nossa base. Trata-se de uma representação visual da Matriz de Distâncias.

O Dendrograma pode ser utilizado para definir quantos grupos existem nos dados que estamos analisando. Observando o dendrograma, podemos estabelecer uma altura de corte e, a partir disso, dividimos os grupos. Na imagem abaixo, se fizermos um corte na altura de 15, temos dois grupos, um com Japão e outro com todos os outros países. Se o corte for na altura de 10, temos três grupos: um com Japão, outro com Marrocos e Angola e um terceiro com Brasil, Argentina e Chile.

![[Pasted image 20250522075956.png]]

**Vantagens**:

- Não é necessário definir o número de cluster a priori;
- Dendrograma é uma forma clara de visualização.

**Desvantagens:**

- Por ter um alto custo de processamento e memória não é adequado para bancos de dados grandes.


**Tópicos Avançados**

Na nossa terceira aula, nos aprofundamos na análise dos Algoritmos Hierárquicos. Vimos quais são suas premissas e como eles podem ser implementados utilizando Python. Sendo a Seleção do Algoritmo de Aprendizagem uma das Etapas Críticas da Análise de Clusters, compreender bem essa família de algoritmos é fundamental. Por isso, recomendo que acesse os conteúdos a seguir:

- **StatQuest: Hierarchical Clustering** - vídeo do canal StatQuest with Josh Starmer, disponível no Youtube.
- **Agrupamento hierárquico** - texto escrito por William Lucena, disponível no Medium.


----

# Clusterização Particionais

Algoritmos de Clusterização Particional buscam organizar os dados em múltiplos grupos, tendo como base as características e a similaridade entre os dados. Para utilizar os algoritmos particionais, é necessário informar desde o início quantos grupos queremos encontrar.

Uma vez que temos nossa base de dados, o algoritmo a ser utilizado e a quantidade de grupos a serem encontrados, podemos aplicar nosso algoritmo de Aprendizagem de Máquina. Aqui é importante estabelecer que a quantidade de grupos será representada por k. Ou seja, quando formos nos referir ao número de cluster que estamos buscando ou que foram encontrados, iremos dizer “queremos encontrar k grupos”, sendo que k pode ser substituído por qualquer valor inteiro maior que 1.

Dada uma base de dados com N observações, os algoritmos Particionais irão separar os dados em k partições de dados, sendo que cada uma dessas partições representa um grupo. Nesse processo, existem duas premissas que devem ser sempre respeitadas:

1. Cada grupo deve conter ao menos um elemento;
2. Cada elemento deve pertencer a um, e, somente a um, cluster.

Outra restrição importante é que o número de grupos k deve ser menor ou igual ao número de observações N na base, ou seja, k ≤ N.

Um dos principais, e talvez o mais famoso, algoritmo de Agrupamento Particional é o k-means (means significa médias, em inglês).

Como o k-means é um algoritmo particional, o primeiro passo para utilizá-lo é definir o valor de k. De forma simplificada, esses são os passos do algoritmo:

1. Escolher k centroides;
2. Atribuir cada observação ao centroide mais próximo;
3. Atualizar a posição de cada centroide como a média de todos os elementos do grupo;
4. Testar se após a atualização os centroides mudaram;
    . Se sim, volta ao passo 2;
    . Se não, encerra.

Do ponto de vista matemático, o k-means busca minimizar a soma da variância intra-cluster:

![[Pasted image 20250523072809.png]]

De forma simplificada, o objetivo desta função é calcular a distância d de um ponto x até a média x̅ do seu próprio cluster, elevada ao quadrado. Essa soma é realizada para todos os elementos dentro de um cluster, resultando na variância intra-cluster de um grupo. Posteriormente, todas essas variâncias são somadas, gerando o valor J, que é o que buscamos minimizar.

Um dos pontos fortes do algoritmo k-means é a sua relativa simplicidade e, até certo ponto, intuição. O que ele faz é otimizar o processo de identificação de grupos usando a definição de agrupamento: os elementos dentro de um grupo devem ser o mais similares possíveis entre si e o mais diferentes possível dos itens em grupos diferentes.

De forma resumida, as **vantagens** do algoritmo k-means são:

- Simples e intuitivo;
- Possui complexidade Linear em todos os parâmetros importantes;
- Funciona bem quando aplicado em cenários práticos;
- Seus resultados têm uma interpretação Simples.
- 

Por outro lado, as **desvantagens** de utilizá-lo podem ser:

- Necessidade de definir o valor de k;
- Sensível a inicialização;
- Limita-se a encontrar grupos globulares e com volumetria parecida;
- Cada item deve pertencer a um único cluster (partição rígida);
- Limitado a atributos numéricos;
- Sensível a outliers.

Em relação a sensibilidade à inicialização, algumas abordagens podem ser aplicadas para reduzir o impacto dessa característica e evitar que o algoritmo fique preso em mínimos locais. As principais delas são:

- **Fazer múltiplas execuções com inicialização aleatória**: aqui, a ideia é ser capaz de encontrar os centroides que mais ocorrem como a versão final para escolha dos grupos.
- **Utilizar Agrupamento Hierárquico**: como vimos, o agrupamento hierárquico não performa bem com bases de dados grandes; porém, é excelente para lidar com bases relativamente pequenas. O que podemos fazer aqui é utilizar uma amostra da base de dados para agrupar os dados, calcular os centroides com k grupos e inicializar o k-means, passando esses valores como parâmetros iniciais.
- **Seleção informada**: escolher o primeiro centroide de forma aleatória ou como o centro dos dados, a média de todos os elementos do grupo e, em seguida, de forma iterativa, escolhe o próximo valor central como sendo o elemento mais distante dos já selecionados.

Já para lidar com outliers, a estratégia é utilizar uma versão alternativa do k-means, o **k-medóides.** Neste algoritmo, um centroide não é a média do grupo, mas um objeto do cluster que está próximo desse valor. Com isso, mesmo que um outlier puxe o valor da média, o centro do grupo estará fixo, próximo à parte mais densa do cluster; afinal, o medoide, equivalente ao centroide no k-means, é o objeto do cluster que está mais próximo do valor médio do grupo.

Além de ser menos sensível a outliers, o k-medoides também pode ser aplicado a bases de dados com atributos categóricos e possui convergência assegurada com qualquer medida de similaridade. Sua principal desvantagem é que ele tem complexidade quadrática no número de observações (N).

Por fim, para lidar com a escolha do k, é possível utilizar alguns índices de validação internos e/ou relativos, para tentar encontrar o valor que otimiza a função objetivo.


- **Aula 13: Clustering** - vídeo do canal WiMLDS São Paulo, disponível no Youtube.
- **Capítulo 4: Análise de Clusters** - texto escrito por Felipe Micail da Silva Smolski, disponível no site Smolski.
- **Visualizing K-Means Clustering** - texto disponível no site Naftali Harris.
- **StatQuest: K-means clustering** - vídeo do canal StatQuest with Josh Starmer, disponível no Youtube.

----
## Clusterização Baseada em Densidade

Algoritmos de Clusterização Baseados em Densidade são um tipo de técnica de aprendizado não supervisionado. Eles identificam grupos, ou clusters, nos dados baseados nas seguintes premissas:

- Um grupo é uma região do espaço com uma **alta densidade** de pontos;

- Grupos são separados uns dos outros por regiões de **baixa densidade** de pontos;

- Pontos em regiões de **baixa densidade** são considerados outliers ou **ruído.**

Podemos visualizar o funcionamento desse tipo de algoritmo imaginando uma avenida na região central de uma metrópole, como a cidade de São Paulo. Suponha que nos dois lados de uma avenida qualquer existam dois grupos de pessoas aguardando o semáforo para atravessar a rua. Se aplicássemos um algoritmo baseado em densidade, encontraríamos dois clusters, o primeiro em uma das calçadas e o segundo na outra, que são regiões com uma alta concentração de elementos. Nesse caso, esses elementos são pessoas. Sabemos que são dois grupos, pois eles estão separados um do outro por uma área de baixa densidade, no caso a avenida, na qual os elementos que estamos agrupando, pessoas, são raros ou inexistentes.

Os Algoritmos deste tipo são capazes de encontrar grupos de formas arbitrárias, que também são chamados de _clusters naturais_. Esses grupos podem ser representados, por exemplo, por rios ou cadeias de montanhas, que são áreas contínuas e conectadas entre si, enquanto divergem da paisagem ao redor. Nesse sentido, uma técnica de Clusterização Baseada em Densidade poderia ser utilizada, por exemplo, para o mapeamento de alguma área ou estrutura sinuosa, o que para Algoritmos Hierárquicos ou Particionais seria impossível.

De forma geral, os Conceitos Fundamentais desses algoritmos são bastante parecidos. Aqui, iremos explorar com maior profundidade as características do DBSCAN, mas as premissas e nomenclaturas apresentadas podem ser extrapoladas para vários outros algoritmos dessa classe.

DBSCAN é a sigla de Density-Based Spatial Clustering of Applications with Noise, em português seria algo como Agrupamento Espacial Baseado em Densidade de Aplicações com Ruído. O nome parece bastante promissor. Vamos entender como esse algoritmo funciona. Em primeiro lugar, existem dois Parâmetros que precisamos definir antes de aplicar o algoritmo, são eles:

- **MinPts:** Número mínimo de pontos que deve estar em volta de um ponto para que ele seja considerado um ponto central.
- **Eps:** Raio que deve ser observado em torno de um ponto para buscar por pontos próximos.

No parágrafo anterior, observamos um conceito novo, o de Ponto Central; iremos definir melhor o que isso significa, bem como introduzir outros dois conceitos importantes:

- **Ponto Central (Core Point):** Um ponto, ou observação, que possui um número mínimo de pontos (MinPts) ao seu redor dentro de um raio Eps. São pontos que estão no interior de um cluster.
- **Ponto de Fronteira (Border Point):** Um ponto, ou observação, que não possui em torno de si o número mínimo de pontos (MinPts), porém está dentro do raio Eps de, ao menos, um ponto central.
- **Ruído (Noise):** Pontos que não são Centrais, nem de Fronteira. São observações muito diferentes e que estão isoladas das demais. Podem ser consideradas como outliers e não serão atribuídas a um cluster.

 O algoritmo do DBSCAN pode ser construído da seguinte forma:

1. Percorra a base de dados e marque os objetos como Core, Border ou Noise.
2. Elimine os objetos rotulados como Noise.
3. Conecte os objetos Core vizinhos (2 objetos são vizinhos se estiverem dentro do raio Eps um do outro).
4. Faça cada componente conexo resultante ser um cluster.
5. Atribua cada Border ao cluster de um de seus Core associados.


➡️ De forma resumida, as vantagens do algoritmo DBSCAN são:

- Não necessita do número de clusters a priori;
- Capacidade de encontrar grupos com formatos arbitrários;
- Por definição, consegue lidar bem com outliers.
- Possui apenas dois parâmetros, MinPts e Eps.

➡️ Por outro lado, suas desvantagens são:

- Muito sensível aos parâmetros MinPts e Eps;
- Não é capaz de agrupar corretamente dados que possuem clusters com densidades diferentes;
- Se a escala dos dados não for conhecida, definir o valor de Eps (raio) pode ser muito difícil.

Para lidar com a dificuldade do DBSCAN de lidar com bases de dados com densidades diferentes, outro algoritmo Baseado em Densidade foi desenvolvido. Ele é chamado de OPTICS, que é a sigla de Ordering Points To Identify Cluster Structure, em português seria algo como “Ordenando Pontos Para Identificar a Estrutura do Grupo”.

A premissa principal do OPTICS é que o parâmetro Eps não é fixo, ele é definido com base no contexto. Um contexto aqui é uma região e, nessa região, a distância de todos os pontos para todos os pontos é calculada e, a partir disso, o conceito de longe e perto é construído, tendo como entrada principal os próprios dados. Muito legal!


Considerando o quão crucial a etapa de Seleção de Algoritmo de Aprendizagem é para a Análise de Clusters, é importante que nos familiarizemos com a forma como cada um desses modelos funciona e se comporta. Quanto mais claro tivermos isso, mais capazes seremos de determinar qual algoritmo se ajusta melhor aos dados. Para aprofundar-se no estudo do tema e conhecer outras formas de abordar esses conceitos, recomendo acessar os seguintes conteúdos:

- **Agrupamento baseado em densidade** - texto escrito por Elvis Melo, João Mendes, Ayrton Everton, Thiago de Oliveira, Matheus Carvalho, disponível no Medium.
- **Combinação de Clusterização Baseada em Densidade com Análise Espectral Singular de Séries Temporais: Uma Aplicação à Velocidade do Vento** - texto escrito por Keila Mara Cassiano, Reinaldo Castro Souza e José Francisco Moreira Pessanha, disponível no site da Marinha.
- Entendendo DBSCAN - texto escrito por Gabriel Monteiro, disponível no Medium.

---
## Avaliação de Grupos

O processo de Análise de Clusters pode ser dividido em quatro partes, que podem ser revisitadas a qualquer momento, sendo elas: Representação de Dados, Selecionar Medida de Proximidade, Selecionar Algoritmo e Avaliar os Grupos Encontrados. Nosso foco agora será na última etapa: **Avaliar os Grupos Encontrados**.

_A validação das estruturas de grupos encontrados é a parte mais difícil e frustrante da Análise de Clusters. Sem um esforço consciente nessa direção, a Análise de Clusters continuará a ser uma Arte das Trevas, acessível apenas aos escolhidos, que possuem experiência e grande coragem._ (Jain e Dubes, Algorithms for Clustering Data, 1988)

Essa é uma frase bastante utilizada por quem costuma trabalhar com agrupamento e sumariza bem o desafio que é a parte final da Análise de Clusters. Como medir o resultado de um procedimento em que, a priori, não possui uma resposta correta? Embora isso seja verdade, é importante que nós sejamos capazes de, por algum meio, encontrar uma resposta numérica para isso. Em todo caso, a primeira abordagem aqui é a análise qualitativa humana. Ter contato com especialistas do Negócio ou Domínio é essencial para garantir que os grupos encontrados de fato fazem sentido e podem ser aplicados ao problema real.

Além disso, existem diferentes procedimentos para avaliar de maneira **objetiva** e **quantitativa** os resultados da Análise de Clusters. Cada um desses procedimentos pode nos ajudar a responder uma ou mais questões do tipo:

- Encontramos grupos de fato?
- Esses conjuntos são pouco usuais ou facilmente encontrados ao acaso?
- Qual a qualidade (relativa ou absoluta) do resultado do agrupamento?
- Qual é o número natural/mais apropriado de clusters?

Para validarmos de maneira **quantitativa** o resultado encontrado podemos utilizar métricas e índices. Esses índices podem ser divididos em 3 categorias:

**1. Critérios Relativos:** Avaliam qual dentre duas ou mais estruturas de grupos é melhor sob algum aspecto. Tipicamente são critérios internos capazes de quantificar a qualidade relativa.

Um exemplo dessa métrica é a Silhueta (Silhouette Score). Ela é utilizada para avaliar a qualidade dos grupos com base em quão compactos eles são e o quão separados estão uns dos outros. Para calcular essa métrica, utilizamos dois parâmetros:

- **a** = Distância Média Intra-Cluster, distância média de um ponto para todos os outros pontos do mesmo cluster
- **b** = Distância Média para o Cluster mais Próximo, distância média de um ponto para todos os pontos do cluster mais próximo

Para cada ponto ou observação, o valor da silhueta é determinado pela seguinte fórmula:

![[Pasted image 20250526193120.png]]
O valor da silhueta varia entre -1 e 1. Se o valor for 1 significa que o cluster é denso e bem separado, -1 implica que o ponto foi atribuído ao cluster errado.

**2. Critérios Internos:** Eles avaliam o grau de compatibilidade entre a estrutura dos grupos em avaliação e os dados, utilizando somente os próprios dados como referência. Um exemplo dessa abordagem é o Método do Cotovelo (Elbow Method).

A proposta do Método do Cotovelo é determinar o número ideal de 'k' para uma base de dados. Em algoritmos como o K-means, o objetivo da clusterização é identificar maneiras de segmentar os dados de modo que as observações próximas se assemelhem entre si e se distingam ao máximo das observações em outros grupos. Para o Método do Cotovelo a quantidade o valor de k ideal é aquele que minimiza a **soma dos quadrados das distâncias intra-clusters** (ou em inglês, within clusters sum of squares, WCSS)

![[Pasted image 20250526193139.png]]

**3. Critérios Externos:** Eles avaliam o grau de correspondência entre a estrutura dos grupos (seja em partição ou hierarquia) em avaliação e uma informação prévia, apresentada na forma de uma solução de agrupamento esperada ou já conhecida.

Neste contexto, podemos citar o Rand Index como exemplo. Esse índice serve como uma medida de similaridade entre dois conjuntos. Ele avalia a eficácia de um algoritmo de agrupamento em segmentar a base de dados, considerando que as classes corretas são parcialmente ou totalmente conhecidas. O cálculo desse índice é realizado da seguinte maneira:

![[Pasted image 20250526193200.png]]

Sendo que:

- **a** = Número de pares da mesma classe e do mesmo cluster.
- **b** = Número de pares da mesma classe e de clusters distintos.
- **c** = Número de pares de classes distintas e do mesmo cluster.
- **d** = Número de pares de classes e clusters distintos.

**Tópicos Avançados**

Antes, discutimos os diversos tipos de agrupamento, o que nos proporcionou diversas opções para a terceira etapa da Análise de Cluster: a seleção do Algoritmo de Aprendizagem. Agora, exploramos a fase final desse processo e nos aprofundamos em alguns métodos para avaliar os grupos identificados. Se desejar se aprofundar ainda mais sobre validação de clusters, recomendo a consulta aos seguintes conteúdos:

- **9 tipos de Distâncias:**  
    [https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa) **(Acesso: em 03/02/2023)**

- **Como definir o número de Cluster no K-means:**

[https://medium.com/pizzadedados/kmeans-e-metodo-do-cotovelo-94ded9fdf3a9](https://medium.com/pizzadedados/kmeans-e-metodo-do-cotovelo-94ded9fdf3a9) **(Acesso em: 03/02/2023)**

●        **Qualidade de Agrupamento:**  
[https://medium.com/@gilneyjnr/qualidade-de-agrupamentos-ci%C3%AAncia-de-dados-4b1176bef5e5](https://medium.com/@gilneyjnr/qualidade-de-agrupamentos-ci%C3%AAncia-de-dados-4b1176bef5e5)  **(Acesso em: 05/02/2023)**

●        **Métricas de Avaliação de Clusters:**  
[https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)  **(Acesso em: 05/02/2023)**

----

## Projetos de Clusterização

O processo de segmentar e agrupar objetos é semelhante à forma como o cérebro humano processa o mundo. Por exemplo, ao aprendermos uma nova matéria, faz parte do processo separar o conteúdo em partes, para que, de maneira sequencial e estruturada, passemos por toda a jornada de aprendizado sem precisar sermos expostos a todos os conceitos importantes de maneira simultânea.

Como mencionamos em nossa primeira aula, o processo de organizar nossas músicas ou montar um quebra-cabeça também são exemplos de como o conceito de agrupamento está inserido em nosso cotidiano. No entanto, uma vez que nós somos capazes de lidar naturalmente com problemas de agrupamento, ou talvez de aprendizado supervisionado, por que precisaríamos recorrer aos algoritmos de aprendizagem de máquina para esse processo?

A Análise de Clusters é o processo de estruturar e expandir essa habilidade humana que é agrupar instâncias parecidas e extrair valor de dados. Por meio da Aprendizagem de Máquina, um subcampo da computação e da Inteligência Artificial, que visa permitir que máquinas aprendam ou sejam capazes de executar tarefas tradicionalmente humanas, tornamos o processo de encontrar grupos em algo escalável, estruturado, mensurável e reprodutível.

A análise de clusters é definida como o processo de “Encontrar grupos de objetos de tal forma que em um grupo sejam similares (ou relacionados) um ao outro e diferentes de (ou não relacionados) a objetos em outros grupos” (Tan et al., 2006).

Indo além dos conceitos e fases da Análise de Clusters que visitamos na aula 2, podemos expandir nossa perspectiva e passar a tratar o desafio de clusterização como um Projeto de Ciência de Dados. Ao fazermos isso, somos apresentados a uma nova forma de lidar com o desafio de resolver problemas usando dados.

![[Pasted image 20250526203033.png]]

Vamos abordar uma Metodologia para Organizar Projetos de Ciência de Dados, o CRISP-DM, sigla que significa _Cross Industry Standard Process for Data Mining_, ou em português, Processo Padrão Inter-Indústrias para Mineração de Dados. CRISP-DM tem o objetivo de orientar o desenvolvimento de projeto de Ciência de Dados, organizando esse fluxo em conjunto de etapas e tarefas relacionadas e complementares. Ele também pode ser interpretado como Modelo de Processo que descreve o ciclo de vida de um projeto de Ciência de Dados.

O primeiro aspecto a ser destacado sobre o CRISP-DM é que ele é cíclico. Repare que em volta da figura 1, as fases estão envoltas por um círculo e setas. O objetivo aqui é deixar claro que é um processo contínuo de aprendizagem com os dados e interação com clientes e negócios.

Na parte interna do diagrama estão listadas as seis fases da Metodologia, tendo os dados ao centro. Antes de detalharmos cada fase, um ponto de extrema relevância é estar ciente de que é possível, necessário e por vezes obrigatório se mover entre as fases de maneira não linear, indo e voltando até que o resultado faça sentido e seja possível avançar, ciente de que a depender do que ocorrer em seguida, talvez seja necessário voltar mais algumas vezes. No diagrama estão listadas como setas as rotas mais comuns. Porém, a qualquer momento é possível navegar entre qualquer uma das fases. Ter essa consciência é fundamental para entregar projetos de Ciência de Dados de forma correta, sustentável e responsável.

O processo do CRISP-DM começa com a fase de **Contextualização, ou Entendimento de Negócio**: é nessa fase que a pessoa com conhecimentos de Ciência de Dados irá interagir com seu cliente para entender o problema e o negócio e começar a definir objetivos do projeto. A etapa seguinte é a fase de **Entendimento dos Dados**: esse momento é quando se começa a interagir de fato com os dados. Aqui é quando iremos coletar e analisar os dados, analisando sua qualidade e adequação à tarefa proposta. A próxima etapa é a de **Preparação de Dados**, bastante conhecida como Data Prep (Data Preparation): é nessa fase que os dados serão selecionados e tratados para serem aplicados a um modelo de Aprendizagem de Máquina. Nessa etapa, iremos garantir que o formato dos dados está adequado, tratar outliers, combinar variáveis, agregar tabelas e organizar uma base Tidy na qual o processo de modelagem será feito. Se quisermos fazer um paralelo com as fases da Análise de Clusters, essas três primeiras etapas poderiam fazer parte do que chamamos de **Representação do Problema.**

Modelagem é a etapa seguinte no fluxo do CRISP-DM. Em geral, essa é parte que mais entusiasma Cientistas e Analistas de Dados mais anseiam. Aqui, definimos que tipo de modelo utilizaremos, quais algoritmos iremos testar, estruturar um processo de teste e aplicar os modelos escolhidos na nossa base de dados. Quando estivermos satisfeitos com os resultados, passamos para a próxima etapa: Avaliação. Seguindo com nossa ideia de comparar Análise de Clusters e CRISP-DM, a etapa de Modelagem inclui as partes de **Selecionar Métrica de Proximidade** e **Seleção do Algoritmo**.

Na etapa de Avaliação iremos analisar os resultados obtidos, apresentar as métricas de teste, revisar o processo e discutir com nosso cliente qual o próximo passo: queremos voltar à alguma fase anterior ou vamos para o momento da Entrega, ou Deploy? Essa fase do CRISP-DM é equivalente à parte de **Avaliar Grupos Encontrados** da Análise de Clusters.

A última etapa do CRISP-DM é a fase de entrega ou Deploy. É nesta etapa que o modelo será entregue para o cliente e implantado, caso faça sentido. De certa forma esse momento marca o fim do processo; contudo, em muitos casos, a pessoa responsável por desenvolver as análises também seja responsável por implantar e acompanhar o modelo em produção. Este é o início de outros ciclos e requer um novo processo. A última parte do CRISP-DM não possui uma equivalência direta com a Análise de Cluster.

Como comentado anteriormente, o CRISP-DM é um ciclo, a partir da entrega e implantação do modelo, podemos ter que voltar a trabalhar com a mesma base de dados para criar uma nova solução, podemos ter que revisar o modelo entregue ou simplesmente seguir para um outro projeto. O importante é ter em mente que as etapas de um projeto de Ciência de Dados não são lineares,. É sempre possível voltar para as fases anteriores, quando houver necessidade, e é fundamental estar revisitando dados e projetos antigos para garantir que nossa estratégia de dados esteja atualizada e o máximo de oportunidades esteja sendo aproveitado.

**Tópicos Avançados**

Caso queira se aprofundar em metodologias e estratégias para gestão de projetos, passando pelo CRISP-DM e indo para outras abordagens, recomendo os conteúdos abaixo:

- **Documentação completa para uso do CRISP-DM:**  
    [https://www.ibm.com/docs/pt-br/spss-modeler/18.4.0?topic=dm-crisp-help-overview](https://www.ibm.com/docs/pt-br/spss-modeler/18.4.0?topic=dm-crisp-help-overview)  **(Acesso em 03/02/2023)**
- **Repositório de Bases de Dados:** [https://archive.ics.uci.edu/ml/datasets.php](https://archive.ics.uci.edu/ml/datasets.php) **(Acesso em 03/02/2023)**

●        **Gerenciamento de Projetos de Data Science com CRISP:** [https://medium.com/data-hackers/project-data-science-pds-39d5a78e058a](https://medium.com/data-hackers/project-data-science-pds-39d5a78e058a) **(Acesso em 05/02/2023)**

●        **Revisão de Metodologias para Projetos de Ciência de Dados:** [https://abracd.org/gerenciando-projetos-de-data-science-uma-breve-revisao/](https://abracd.org/gerenciando-projetos-de-data-science-uma-breve-revisao/) **(Acesso em 05/02/2023)**

-----
Ao longo das últimas aulas, aprendemos bastante sobre Clusterização, Aprendizado Não Supervisionado e Projetos de Ciência de Dados. Passamos pela definição do que é Clusterização, entendemos o que é Análise de Clusters, conhecemos os principais paradigmas de Agrupamento e os algoritmos mais comuns que são utilizados. Vimos também como estruturar um projeto de Ciência de Dados e como aplicar em vários contextos os algoritmos de Clusterização. Bastante coisa, hein?

Ao chegarmos em nossa última aula iremos falar um pouco sobre os desafios e possibilidades da Disciplina de Clusterização. Começamos com o desafio de Seleção de Variáveis. Essa é uma área bastante estudada e madura quando se trata de selecionar _features_ para modelos de Aprendizagem Supervisionada, uma vez que é possível, a partir da variável resposta, calcular o impacto que cada variável de nossa base tem sobre ela. Quando se trata de Clustering porém, o cenário tende a ser bem diferente.

Algumas pessoas questionam, inclusive, se existe a necessidade de se fazer seleção de variáveis no contexto de Aprendizagem Não Supervisionada. O que se percebe até o momento é que sim. Isso porque quanto maior o número de variáveis para um processo de Aprendizado, maior é a probabilidade de que tenhamos atributos que sejam redundantes, irrelevantes ou mesmo que insiram ruído em nossa análise.

Para lidar com esses desafios, duas abordagens têm sido utilizadas. A primeira delas são os chamados Métodos de Filtro (Filter Methods). Nessa abordagem, as variáveis são comparadas em relação a algum critério antes de serem submetidas ao modelo. Por exemplo, podemos remover variáveis que estejam muito correlacionadas entre si, ou que possuam uma variação muito baixa ou qualquer outro critério que olhe para a variável ou a relação dela com alguma variável.

Uma segunda abordagem recebe o nome de Métodos de Envelope (Wrapper Methods). Nessa estratégia, um subconjunto das variáveis é selecionado, passa pelo modelo e é avaliado sob alguma métrica de qualidade, de forma bem parecida com o que ocorre no método do cotovelo. A estratégia mais simples aqui é aplicar esse processo para todas as combinações de variáveis possíveis, mas isso é bastante custoso e, em muitos casos, impossível de ser feito em tempo útil. Por isso existem ao menos duas abordagens iterativas para mitigar esse problema:

- **Forward Selection:** É um processo iterativo que inicia com zero variáveis e, a cada iteração, adiciona uma nova variável ao modelo até que adicionar variáveis deixe de fazer diferença.
- **Backwards Elimination:** Nesse processo, o modelo começa com todas as variáveis e, a cada iteração, a variável menos relevante é removida. Esse processo se repete até que remover variáveis deixe de melhorar o modelo.

Outro desafio importante é em relação às Variáveis Categóricas. A maioria dos algoritmos de Aprendizagem de Máquina lida com dados numéricos, porém outros tipos de dados têm se tornado cada vez mais comuns. Nesse sentido, ter um algoritmo ou uma estratégia que performe bem em dados categóricos é essencial. Além de selecionar a métrica de distância mais adequada para esse tipo de dados uma tentativa de mitigar esse problema é o algoritmo k-modes (_modes_ é moda em inglês), construído sob os mesmos princípios do k-means, porém pensado para dados categóricos e fazendo uso da moda ou em vez da média para definir os centróides de cada grupo.

Por fim, os algoritmos de Clusterização também podem ser utilizados para agrupar Dados Não-Estruturados, como imagem, texto, áudio e stream (fluxo de dados). Nesse contexto, é necessário pensar em formas de mapear essas instâncias de dados para o contexto de dados estruturados, como gerar features com características do som de uma música em forma de tabela (dado Tidy) ou propondo algoritmos de clusterização mais complexos para lidar com estruturas de dados variadas. Dados Não Estruturados são cada vez mais comuns e a demanda por ferramentas e profissionais que saibam utilizar esse tipo de dado para extrair valor será cada vez maior.

Por fim, seguem alguns links que podem ser úteis para sua evolução em Ciência de Dados e Agrupamento:

**Algoritmos:** Existem várias formas de pensar em como organizar os dados que estamos analisando. Além dos algoritmos que vimos ao longo do curso, existem muitos outros e, a depender do problema com o qual estamos trabalhando, faz sentido que a gente passe a utilizar modelos mais complexos ou com um tipo de viés diferente. A página do scikit-learn pode ser um bom início para começar a dar os próximos passos:

**Redução de Dimensionalidade/Visualização:** Em alguns momentos nós passamos rapidamente pelo PCA para visualizarmos em duas dimensões o resultado do agrupamento. Faz sentido que a gente se aprofunde nesse tópico porque, além da Redução de Dimensionalidade e da utilização para Visualização, essa técnica também pode ser usada para análise descritiva e vai muito adiante do clustering. Alguns exemplos desse tipo de técnica são o próprio PCA e o T-SNE.

**Kaggle:** Encontrar materiais e notebooks de outras pessoas pode ser uma forma de evoluir em nossa aprendizagem. O Kaggle é uma plataforma em que desafios são apresentados e a comunidade tenta criar modelos de aprendizado de máquina para encontrar a melhor solução, às vezes com prêmios em dinheiro para os primeiros colocados. Além disso, nela é possível encontrar bases de dados, análises da comunidade, desde análise descritiva até algoritmos de Redes Neurais e Deep Learning.

**Tópicos Avançados**

Caso tenha interesse em se desenvolver ainda mais em Clusterização, recomendo que acesse os materiais listados abaixo para expandir seus conhecimentos. Além dos links para artigos, recomendo também dois livros que podem fazer bastante diferença em sua formação, “Análise Multivariada de Dados” e “Mãos à Obra: Aprendizado de Máquina”.

- **Sklearn:** [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html) **(Acesso em 03/02/2023)**
- **PCA (vídeo):** [https://youtu.be/pqRWzgkYDr4](https://youtu.be/pqRWzgkYDr4) **(Acesso em 03/02/2023)**
- **PCA (artigo):** [https://arxiv.org/pdf/1804.02502.pdf](https://arxiv.org/pdf/1804.02502.pdf) **(Acesso em 03/02/2023)**
- **Kaggle:** [https://www.kaggle.com/](https://www.kaggle.com/) **(Acesso em 03/02/2023)**
- **Análise Multivariada de Dados** - Joseph F. Hair Jr. (Autor), William C. Black (Autor), Barry J. Babin (Autor), Rolph E. Anderson (Autor), Ronald L. Tatham (Autor), Maria Aparecida Gouvêa Adonai Schlup Sant’Anna (Tradutor).
- **Mãos à obra: aprendizado de máquina com Scikit-Learn & TensorFlow** - Aurélien Géron

●        **Seleção de Variáveis para Agrupamento:**  
[https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_613](https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_613) **(Acesso em 05/02/2023).**

●        **Seleção de Variáveis de Agrupamento (2):**  
[https://www.public.asu.edu/~huanliu/papers/pakdd00clu.pdf](https://www.public.asu.edu/~huanliu/papers/pakdd00clu.pdf) **(acesso em 05/02/2023)**

**●        Introdução ao Agrupamento de Textos:**  
[https://towardsdatascience.com/a-friendly-introduction-to-text-clustering-fa996bcefd04](https://towardsdatascience.com/a-friendly-introduction-to-text-clustering-fa996bcefd04) **(Acesso em 05/02/2023)**

**●        Agrupamento de Texto utilizando k-means:**  
[https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html) **(Acesso em 05/02/2023)**

**●        Estado da Arte em Agrupamento de Fluxo de Dados:**  
[https://bdataanalytics.biomedcentral.com/articles/10.1186/s41044-016-0011-3](https://bdataanalytics.biomedcentral.com/articles/10.1186/s41044-016-0011-3) **(Acesso em 05/02/2023)**

**●        Levantamento de Agrupamento de Fluxo de Dados:**  
[https://dl.acm.org/doi/10.1145/2522968.2522981](https://dl.acm.org/doi/10.1145/2522968.2522981) **(Acesso em 05/02/2023)**

**●        K Modes para Agrupamento de Dados Categóricos:**  
[https://www.analyticsvidhya.com/blog/2021/06/kmodes-clustering-algorithm-for-categorical-data/](https://www.analyticsvidhya.com/blog/2021/06/kmodes-clustering-algorithm-for-categorical-data/) **(Acesso em 05/02/2023)**