Agora, vamos falar sobre dois modelos de machine learning do tipo MAB para aprendizado por refor√ßo. √Ä medida em que os problemas e desafios ficam mais complexos e diferentes, precisamos de solu√ß√µes mais criativas para resolv√™-los e, neste sentido, o aprendizado por refor√ßo vem como uma alternativa para os tipos cl√°ssicos de machine learning (supervisionado e n√£o supervisionado).

Em geral, quando temos um problema que pode ser resolvido por um algoritmo de aprendizado por refor√ßo, a melhor solu√ß√£o ser√° realmente utilizar um algoritmo desta fam√≠lia, pois nos dar√° o melhor resultado.

A sigla MAB significa Multi Armed Bandit. √â uma classe de algoritmos de machine learning de aprendizado por refor√ßo. Um MAB tenta resolver um problema onde voc√™ possui v√°rias op√ß√µes de escolha e tem como objetivo maximizar a sua recompensa obtida numa escolha vencedora. Surgiu com o ca√ßa-n√≠quel e hoje utilizamos em muitas aplica√ß√µes de mercado.

O grande problema √© que cada arm tem sua pr√≥pria distribui√ß√£o de probabilidade de sucesso, mas n√£o conhecemos essa probabilidade. Ent√£o, o funcionamento vai se dar por exploration e exploitation. Na fase de exploration, o algoritmo busca entender qual arm tem a maior chance de sucesso. Uma vez identificado o melhor arm, come√ßa a fase de exploitation, onde a grande maioria das vezes o melhor arm √© selecionado.

A ideia √© que, uma vez identificada a op√ß√£o que te d√° recompensas mais vezes, voc√™ passa majoritariamente a escolher essa op√ß√£o, at√© que uma outra op√ß√£o se mostre melhor.

![[Pasted image 20250520171328.png]]

O MAB vai usar os resultados da explora√ß√£o inicial para alocar mais pessoas para os arms com melhor desempenho, enquanto aloca menos tr√°fego para arms com baixo desempenho. Em teoria, os algoritmos de MAB devem produzir uma recompensa geral mais alta e uma puni√ß√£o geral menor, ao mesmo tempo em que permitem coletar dados sobre como os usu√°rios interagem com diferentes arms.

![[Pasted image 20250520171713.png]]

Ou seja, a partir do momento que o algoritmo entende qual o melhor caminho, a parte de exploration acaba e come√ßa a fase de exploitation em que a maioria dos usu√°rios recebe o arm que est√° com uma maior recompensa. Entretanto, os outros arms sempre recebem algum tr√°fego, mesmo que pequeno, porque os cen√°rios podem mudar ao longo do tempo.

# Upper Bound Confidence (UCB)

O algoritmo UCB1 √© um algoritmo de reinforcement learning do time MAB. Sua sigla significa Upper Bound Confidence ou limite superior de confian√ßa.

Seu nome tem a ver com a forma como ele funciona. Isso porque, para cada arm, ele ir√° identificar um limite superior de confian√ßa (UCB) que representa o maior palpite sobre o poss√≠vel retorno para esse arm. Em cada itera√ß√£o, o algoritmo escolher√° o arm com o UCB mais alto.

O UCB, para cada arm, √© calculado com base tanto na recompensa m√©dia do arm quanto no n√∫mero de vezes que o arm foi alocado, com a seguinte f√≥rmula:

Escolhe a a√ß√£o com o maior¬†**limite superior de confian√ßa**¬†(recompensa m√©dia + margem de incerteza).

**F√≥rmula**

UCB(a) = M√©dia(a) + ‚àö(ln(t) / N(a))

- Onde:
    - UCB(a) √© o valor UCB para a a√ß√£o¬†_a_.

    - M√©dia(a) √© a m√©dia das recompensas obtidas ao executar a a√ß√£o¬†_a_.

    - t √© o n√∫mero total de vezes que a a√ß√£o¬†_a_¬†foi executada.

    - N(a) √© o n√∫mero de vezes que cada a√ß√£o foi testada.

Conforme mostrado pela f√≥rmula, o resultado do UCB de um arm diminui √† medida em que o n√∫mero de vezes em que o arm for escolhido aumenta, pois o valor de `n` vai levando a fra√ß√£o para zero. Ou seja, mesmo que um arm espec√≠fico esteja ganhando em recompensa m√©dia, o UCB pode ser menor do que um arm menos explorado, com uma recompensa m√©dia um pouco mais baixa. Assim, o algoritmo √© capaz de encontrar o equil√≠brio entre explorar op√ß√µes desconhecidas e explorar a variante vencedora.

No caso do UCB1-Tuned, trocaremos o segundo termo da f√≥rmula, que controla o tamanho da margem de erro de escolha, pelo seguinte termo:

![[Pasted image 20250520173027.png]]

Onde V(n) pode ser dado por:

![[Pasted image 20250520173052.png]]

O UCB1-Tuned tem sido muito utilizado pois, em testes emp√≠ricos, performa muito melhor do que o UCB1!

**T√≥picos avan√ßados**

Existem outros algoritmos de MAB, como o¬†**Thompson Sampling**¬†e o¬†**ùú∫-greedy**. Al√©m disso, existem algumas varia√ß√µes e evolu√ß√µes do MAB, por exemplo, o¬†**MAB ranqueado**, que tenta entender qual o melhor arm em cada posi√ß√£o configurada.

Temos algoritmos de MAB que tentam entender o contexto e at√© mesmo resolver¬†**problemas n√£o lineares**¬†e, em geral, tamb√©m envolvem algoritmos de deep learning.

Seguem os links abaixo para voc√™s acessarem:

[https://towardsdatascience.com/thompson-sampling-fc28817eacb8](https://towardsdatascience.com/thompson-sampling-fc28817eacb8)

[https://towardsdatascience.com/bandit-algorithms-34fd7890cb18#0145](https://towardsdatascience.com/bandit-algorithms-34fd7890cb18#0145)

[https://github.com/alison-carrera/mabalgs](https://github.com/alison-carrera/mabalgs)

[https://github.com/alison-carrera/onn](https://github.com/alison-carrera/onn)