K-Means Ã© um algoritmo de aprendizado nÃ£o supervisionado para clusterizaÃ§Ã£o. Seu nome vem da forma como ele funciona, jÃ¡ que seu objetivo Ã© achar centrÃ³ides para cada cluster, que sÃ£o a mÃ©dia de todas as observaÃ§Ãµes pertencentes Ã quele cluster, fazendo isso para todos os K clusters.

- Cada cluster Ã© representado por umÂ **centrÃ³ide**Â (ponto central).
- Os dados sÃ£o agrupados com base naÂ **distÃ¢ncia**Â (ex: Euclidiana) atÃ© o centrÃ³ide mais prÃ³ximo.

**Objetivo**: Minimizar a variÃ¢nciaÂ _dentro_Â de cada cluster.

![[Pasted image 20250517073918.png]]

CentrÃ³ides nada mais sÃ£o do que pontos no espaÃ§o de variÃ¡veis que temos disponÃ­veis em nosso conjunto de dados, que estarÃ£o no centro do cluster.
### **ğŸ”¹ Como Funciona?**

1. **Escolha de K**: Define-se o nÃºmero de clusters (ex:Â *K=3*).
    
2. **InicializaÃ§Ã£o**: Os centrÃ³ides sÃ£o posicionados com mÃ©todos comoÂ _K-Means++_ (Â Euclidiana para calcular a distÃ¢ncia entre as observaÃ§Ãµes e os centrÃ³ides).
    
3. **AtribuiÃ§Ã£o**: Cada ponto Ã© associado ao centrÃ³ide mais prÃ³ximo.
    
4. **AtualizaÃ§Ã£o**: Os centrÃ³ides sÃ£o recalculados como a mÃ©dia dos pontos do cluster.
    
5. **RepetiÃ§Ã£o**: Passos 3 e 4 sÃ£o repetidos atÃ© a convergÃªncia (centrÃ³ides nÃ£o mudam mais).

![[K-means_convergence.gif]]

Existem 3 hiperparÃ¢metros importantes do K-Means:

- n_clusters - NÃºmero de clusters que o K-Means precisa encontrar.
- max_iter - NÃºmero de iteraÃ§Ãµes atÃ© encontrar o centrÃ³ide ideal.
- n_init - NÃºmero de vezes que o algoritmo repete o processo inteiro de clusterizaÃ§Ã£o.
### **CÃ³digo (Python)**
```python
from sklearn.cluster import KMeans  
import matplotlib.pyplot as plt  
from sklearn.datasets import make_blobs  

# Dados sintÃ©ticos (3 clusters)  
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)  

# Criando e treinando o modelo  
kmeans = KMeans(n_clusters=3)  
kmeans.fit(X)  
labels = kmeans.labels_  

# VisualizaÃ§Ã£o  
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')  
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')  
plt.title("K-Means: 3 Clusters com CentrÃ³ides (X)")  
plt.show() plt.show()
```

![[Pasted image 20250517073348.png]]

**Vantagens**

- **Simples e rÃ¡pido**Â para datasets nÃ£o muito grandes.
- **FÃ¡cil interpretaÃ§Ã£o**Â (centrÃ³ides explicam os clusters).
- **EscalÃ¡vel**Â para muitas amostras.


 **LimitaÃ§Ãµes**

- **Escolha de K**: Requer definirÂ _K_Â manualmente (mÃ©todos comoÂ _Elbow Method_Â ajudam).
- **Clusters esfÃ©ricos**: Assume que os grupos sÃ£o convexos e de tamanho similar.
- **SensÃ­vel a outliers**Â e inicializaÃ§Ã£o aleatÃ³ria.

### **ğŸ’¡ Dicas PrÃ¡ticas**

1. **Padronize os dados**Â (ex:Â `StandardScaler`) para evitar dominÃ¢ncia de features com escalas diferentes.
2. **Use K-Means++**Â (default no scikit-learn) para inicializaÃ§Ã£o inteligente dos centrÃ³ides.
3. **Valide com mÃ©tricas**Â comoÂ _Silhouette Score_Â ouÂ _Inertia_Â (soma das distÃ¢ncias quadradas
4. - Tem um funcionamento simples.
5.  Escala bem para grandes conjuntos de dados.
6. Ã‰ genÃ©rico e nÃ£o paramÃ©trico.
7.  Se adapta bem para novas observaÃ§Ãµes


____

# DBSCAN

DBSCAN Ã© um algoritmo de aprendizado nÃ£o supervisionado para clusterizaÃ§Ã£o. A sigla significa Density Based Spatial Clustering of Applications with Noise (ClusterizaÃ§Ã£o Espacial Baseada em Densidade de AplicaÃ§Ãµes com RuÃ­do) e seu objetivo Ã© encontrar vizinhanÃ§as de observaÃ§Ãµes para formar os clusters. Ele faz isso determinando, para cada observaÃ§Ã£o, um raio de tamanho ğœº, que deve conter um mÃ­nimo de pontos M.

Ele, diferente do K-Means, Ã© um Ã³timo algoritmo para identificar clusters de densidades, formatos e tamanhos diferentes entre si. Vamos entender passo a passo como ele funciona.

![[Pasted image 20250517080939.png]]

### **ğŸ” Como o DBSCAN funciona?**

O algoritmo classifica os pontos emÂ **3 categorias**:

1. **Pontos centrais (core points)**: Pontos com pelo menosÂ `min_samples`Â vizinhos dentro de um raioÂ `eps`.
    
2. **Pontos de borda (border points)**: Pontos dentro do raioÂ `eps`Â de umÂ _core point_, mas sem vizinhos suficientes.
    
3. **RuÃ­dos (noise points)**: Pontos que nÃ£o sÃ£o nemÂ _core_Â nemÂ _border_.
    

**Passos**:

1- Escolha aleatoriamente uma observaÃ§Ã£o nos dados.

2- Trace o raio de tamanho ğœº a partir desse ponto e classifique entre ponto central, de fronteira ou outlier, de acordo com o nÃºmero mÃ­nimo M de pontos na vizinhanÃ§a.

3- Se for um ponto central, ele irÃ¡ determinar um cluster, e todos os pontos de fronteira farÃ£o parte desse cluster. Assim, sucessivamente, atÃ© que nÃ£o haja mais nenhum ponto de fronteira.

4- Se for um ponto outlier, escolha outro ponto atÃ© que ele seja um ponto central, repetindo o passo 3.

5- Repita os dois Ãºltimos passos atÃ© nÃ£o restar nenhum ponto no conjunto de dados.

![[dbscan.gif]]

**Vantagens**

- NÃ£o Ã© necessÃ¡rio escolher o nÃºmero de clusters.
- Funciona muito bem para identificar clusters de tamanhos e formatos diferentes.
- Ã‰ Ã³timo para determinar ruÃ­dos como outliers.

**Desvantagens**

- Determinar o valor de ğœº pode ser difÃ­cil.
- Para um conjunto de dados de alta dimensÃ£o, utilizar a distÃ¢ncia euclidiana pode ser ruim, mas achar uma distÃ¢ncia melhor pode ser difÃ­cil.
- NÃ£o funciona bem para dados muito esparsos.

**exemplo em python**

```python
from sklearn.cluster import DBSCAN  
import matplotlib.pyplot as plt  
from sklearn.datasets import make_moons  

# Dados nÃ£o lineares (formato de luas)  
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)  

# Criando e treinando o modelo  
dbscan = DBSCAN(eps=0.3, min_samples=5)  
labels = dbscan.fit_predict(X)  

# VisualizaÃ§Ã£o  
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap="viridis")  
plt.title("DBSCAN: Clusters nÃ£o lineares e outliers (ruÃ­dos)")  
plt.show() 
```


### **ğŸ’¡ Dicas para Usar DBSCAN**

1. **Padronize os dados**Â (ex:Â `StandardScaler`) para queÂ `eps`Â faÃ§a sentido em todas as dimensÃµes.
    
2. **Teste valores deÂ `eps`**Â com um grÃ¡fico de distÃ¢ncia (_k-distance plot_):

```python
from sklearn.neighbors import NearestNeighbors  
neighbors = NearestNeighbors(n_neighbors=5).fit(X)  
distances, _ = neighbors.kneighbors(X)  
plt.plot(np.sort(distances[:, -1]))  
plt.xlabel("Pontos")  
plt.ylabel("DistÃ¢ncia do 5Âº vizinho mais prÃ³ximo") plt.ylabel("DistÃ¢ncia do 5Âº vizinho mais prÃ³ximo")
```

![[Pasted image 20250517081648.png]]


1. - EscolhaÂ `eps`Â na "dobra" do grÃ¡fico (onde hÃ¡ um "cotovelo").
        
2. **UseÂ `min_samples`Â â‰¥ dimensÃµes dos dados + 1**Â (ex: para 2D,Â `min_samples=3`).

### **ğŸŒ AplicaÃ§Ãµes do DBSCAN**

- **SegmentaÃ§Ã£o de clientes**Â com comportamentos atÃ­picos.
- **DetecÃ§Ã£o de fraudes**Â (outliers).
- **AnÃ¡lise de imagens**Â (aglomerados de pixels).