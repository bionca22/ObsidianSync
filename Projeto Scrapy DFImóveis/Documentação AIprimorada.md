
# Documenta√ß√£o

### Documenta√ß√£o do Projeto: Sistema de Raspagem e Processamento de Dados Imobili√°rios

---

#### **1. Vis√£o Geral do Sistema**
O sistema coleta, processa e analisa dados de im√≥veis a partir do portal DF Im√≥veis, transformando-os em um dataset estruturado para an√°lise. O fluxo principal consiste em:
1. Coleta de URLs de listagens
2. Extra√ß√£o detalhada de informa√ß√µes
3. Tratamento e enriquecimento dos dados
4. Integra√ß√£o com API externa (Google Maps)

---

#### **2. Decis√µes de Projeto Fundamentais**

| Decis√£o | Justificativa | Benef√≠cios |
|---------|---------------|------------|
| Separa√ß√£o de spiders por fun√ß√£o | Isolar responsabilidades | Manuten√ß√£o mais f√°cil, reutiliza√ß√£o de c√≥digo |
| Processamento em etapas distintas | Permitir reprocessamento independente | Flexibilidade para ajustes p√≥s-coleta |
| Uso de Pandas para transforma√ß√µes | Manipula√ß√£o eficiente de dados tabulares | Performance com grandes volumes de dados |
| Modulariza√ß√£o das fun√ß√µes de limpeza | Isolar l√≥gicas espec√≠ficas | Testabilidade e reutiliza√ß√£o |
| Configura√ß√£o via vari√°veis de ambiente | Seguran√ßa e portabilidade | Prote√ß√£o de chaves API, compatibilidade multi-ambiente |

---

#### **3. Estrutura de Pastas Otimizada**

```markdown
üìÇ projeto_imoveis/
‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/          # Dados brutos da coleta
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ processed/    # Dados tratados
‚îú‚îÄ‚îÄ üìÅ docs/             # Documenta√ß√£o
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ spiders/      # Spiders Scrapy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ extrair_urls.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ extrair_info.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ processing/   # Scripts de processamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ processamento.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ tratamento.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ google_api.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/        # Utilit√°rios compartilhados
‚îú‚îÄ‚îÄ üìÑ .env              # Vari√°veis de ambiente
‚îú‚îÄ‚îÄ üìÑ requirements.txt  # Depend√™ncias
‚îî‚îÄ‚îÄ üìÑ scrapy.cfg        # Configura√ß√£o Scrapy
```

---

#### **4. Principais Componentes e Fluxo**

**Fluxo Principal:**
```mermaid
graph TD
    A[Spider: extrair_urls] -->|Gera lista de URLs| B[Spider: extrair_info]
    B -->|Dados brutos| C[Tratamento: limpar_area]
    C --> D[Tratamento: limpar_telefones]
    D --> E[Tratamento: expandir_caracteristicas]
    E --> F[Integra√ß√£o: Google API]
    F --> G[Dataset final]
```

---

#### **5. Classes e Fun√ß√µes Principais**

**A. Spiders (src/spiders/)**
1. `ExtrairUrlsSpider`:
   - **Responsabilidade**: Coletar URLs de im√≥veis paginados
   - **M√©todos-chave**:
     - `start_requests()`: Inicia a navega√ß√£o
     - `parse()`: Processa cada p√°gina e extrai URLs
   - **Sa√≠da**: Lista de URLs em formato JSON/TXT

2. `ExtrairInfoSpider`:
   - **Responsabilidade**: Extrair detalhes de cada im√≥vel
   - **M√©todos-chave**:
     - `parse()`: Extrai 30+ atributos por im√≥vel
   - **Padr√µes de extra√ß√£o**: 
     - CSS Selectors para dados estruturados
     - Express√µes regulares para dados complexos
   - **Sa√≠da**: CSV com dados brutos

---

**B. M√≥dulo de Processamento (src/processing/)**

1. `processamento.py`:
   ```python
   def limpar_area(df, coluna: str) -> pd.DataFrame:
       """Padroniza colunas de √°rea removendo unidades e convertendo para float"""
       # Implementa√ß√£o detalhada...
   
   def limpar_telefones(texto: str) -> list:
       """Extrai e padroniza n√∫meros de telefone usando regex"""
   
   def expandir_caracteristicas(df, coluna: str) -> pd.DataFrame:
       """Transforma caracter√≠sticas em colunas bin√°rias one-hot"""
   ```

2. `tratamento.py`:
   - **Fun√ß√£o**: Orquestrar todo o pipeline de limpeza
   - **Etapas**:
     1. Carregamento de dados brutos
     2. Aplica√ß√£o sequencial das fun√ß√µes de limpeza
     3. Convers√£o de tipos de dados
     4. Expans√£o de caracter√≠sticas

3. `google_api.py`:
   ```python
   def get_address(latitude: float, longitude: float) -> str:
       """Obt√©m endere√ßo formatado a partir de coordenadas"""
       # Usa Google Maps Geocoding API
   ```

---

#### **6. Padr√µes de Qualidade de Dados**

| Coluna | Tipo | Transforma√ß√£o Aplicada |
|--------|------|------------------------|
| area_util | float | Remo√ß√£o de " m¬≤", substitui√ß√£o de v√≠rgulas |
| telefone | list[str] | Extra√ß√£o via regex, remo√ß√£o de duplicados |
| caracter√≠sticas | binary columns | One-hot encoding de features |
| total_andares_empr | float | Remo√ß√£o de texto, tratamento de nulos |
| coordenadas | string | Geocodifica√ß√£o reversa via API |

---

#### **7. Melhores Pr√°ticas Implementadas**

1. **Resili√™ncia na Extra√ß√£o**:
   - Valores default para dados ausentes
   - Tratamento de m√∫ltiplos formatos num√©ricos
   ```python
   .replace("N√£o encontrado", np.nan)
   .str.replace(" m¬≤", "")
   ```

2. **Normaliza√ß√£o Consistente**:
   - Padroniza√ß√£o de unidades de medida
   - Convers√£o expl√≠cita de tipos de dados
   ```python
   df["quartos"] = df["quartos"].astype(int)
   ```

3. **Expans√£o Estruturada de Features**:
   ```python
   # Antes: "Piscina, Varanda, Garagem"
   # Depois: colunas bin√°rias independentes
   ```

4. **Gest√£o de Recursos Externos**:
   - Uso de vari√°veis de ambiente para chaves API
   - Cache impl√≠cito em chamadas de geocodifica√ß√£o

---

#### **8. Instru√ß√µes de Execu√ß√£o**

**Pipeline Completo:**
```bash
# Coletar URLs
scrapy crawl extrair_urls -o data/raw/urls.json

# Converter URLs para TXT
python utils/convert_to_txt.py

# Coletar dados dos im√≥veis
scrapy crawl extrair_info -o data/raw/imoveis.csv

# Processar e limpar dados
python src/processing/tratamento.py
```

**Vari√°veis de Ambiente:**
```ini
# .env
GOOGLE_API_KEY=your_api_key
MAX_CONCURRENT_REQUESTS=5
```

---

#### **9. Recomenda√ß√µes para Evolu√ß√£o**

1. **Implementar Testes Unit√°rios**:
   - Validar fun√ß√µes de limpeza com casos extremos
   - Testar padr√µes regex com diferentes formatos de telefone

2. **Adicionar Monitoramento**:
   - Logging estruturado com n√≠veis de severidade
   - Tracking de qualidade de dados (ex: % de valores nulos)

3. **Otimizar Performance**:
   - Paraleliza√ß√£o do processamento com Pandas chunks
   - Cache de resultados da API Google

4. **Aprimorar Resili√™ncia**:
   - Retry autom√°tico para falhas de geocodifica√ß√£o
   - Valida√ß√£o de schema dos dados extra√≠dos

5. **Containeriza√ß√£o**:
   ```dockerfile
   FROM python:3.10-slim
   WORKDIR /app
   COPY requirements.txt .
   RUN pip install -r requirements.txt
   COPY . .
   CMD ["python", "src/processing/tratamento.py"]
   ```

---

### Resumo Executivo
Este projeto implementa um pipeline ETL completo para dados imobili√°rios, destacando-se pela:
- **Arquitetura modular** com separa√ß√£o clara de responsabilidades
- **Robustez no tratamento** de dados semi-estruturados
- **Integra√ß√£o estrat√©gica** com servi√ßos externos
- **Escalabilidade projetada** para grandes volumes de dados

A estrutura proposta permite f√°cil adapta√ß√£o para novos portais imobili√°rios e fornece bases s√≥lidas para an√°lises de mercado e tomada de decis√£o baseada em dados.