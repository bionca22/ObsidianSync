# Requirements

import pandas as pd
import numpy as np
import os
import re
from google_api import get_address


# Extrair URLs

```spiders/extrair_urls.py```

```bash
scrapy crawl extrair_urls -o json_txt/urls.json && python json_txt/json_txt.py
```

>[!info] 
>Esse comando roda a spider "extrair_urls" e se nÃ£o der erros, roda o script json_txt.py para criar o arquivo links_imoveis.txt

```python
base_url = "https://www.dfimoveis.com.br/venda/df/ceilandia/ceilandia-norte/casa"
```

A variÃ¡vel ```base_url``` Ã© a pagina onde serÃ£o extraÃ­dos os links dos anÃºncios. 
a funÃ§Ã£o ```def_parse() ```  faz um loop na pagina percorrendo da pagina 1 Ã  ultima disponÃ­vel, extraindo os links da seguinte forma:

```python
links_imoveis = [
            f"https://www.dfimoveis.com.br{link}" for link in links if link.startswith("/imovel/")
        ]

```


![[brave_H9y2Xu1y6S.png]]
![[brave_wZuPfeqMY6.png]]

>[!info]
O resultado Ã© um arquivo json com os links extraidos separados por pÃ¡gina

# Formatar Json para txt com um link por linha

```json_txt.py``` -> Formata o arquivo Json gerado na etapa anterior para ficar um link por linha no arquivo ```links_imoveis.txt```

# Extrair info

```extrair_info.py```


Criou-se um data frame ```df``` com os dados do arquivo ```links_imoveis.txt``` em uma coluna chamada "links"

```python
df = pd.read_csv("./json_txt/links_imoveis.txt", header=None, names=["links"]) 
```

Carregou data frame em uma lista ```start_urls``` 


A funÃ§Ã£o ```parse()``` faz um loop nos itens da lista ```start_urls``` raspando os dados dos seletores css.

Regex Ã© utilizado para formatar valores extraÃ­dos (remover espaÃ§os, filtrar caracteres, procurar palavras especÃ­ficas, cortar strings...)


> [!info]
> As configuraÃ§Ãµes de raspagem podem ser editadas no arquivo ```settings.py```.
> Verificar a documentaÃ§Ã£o do scrapy!
> 
> Essas configuraÃ§Ãµes sÃ£o importantes para nÃ£o prejudicar o site raspado com requisiÃ§Ãµes em excesso. Nem ter o IP banido do servidor.
> 
> Essas configuraÃ§Ãµes vÃ£o regular o delay, a frequÃªncia e o numero de requisiÃ§Ãµes simultÃ¢neas (dentre outras opÃ§Ãµes mais complexas como "middlewares")



Essa spider Ã© executada com o seguinte comando:

```bash
scrapy crawl extrair_info -o resultados/imoveis.csv
```

apÃ³s a execuÃ§Ã£o Ã© gerado um arquivo na pasta resultados com nome ```imoveis.csv```

# ðŸ“‚Estrutura das Pastas 
â”œâ”€â”€ __init__.py
â”œâ”€â”€ items.py
â”œâ”€â”€ middlewares.py
â”œâ”€â”€ pipelines.py
â”œâ”€â”€ settings.py
â””â”€â”€ spiders
    ==â”œâ”€â”€ extrair_info.py==
    ==â”œâ”€â”€ extrair_urls.py==
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ json_txt
    â”‚   â”œâ”€â”€ json_txt.py
    â”‚   â”œâ”€â”€ links_imoveis.txt
    â”‚   â””â”€â”€ urls.json
    â”œâ”€â”€ resultados
    â”‚   â”œâ”€â”€ imoveis.csv
    â”‚   â””â”€â”€ imoveis.ods
    ==â””â”€â”€ tratar==
        ==â”œâ”€â”€ google_api.py==
        ==â”œâ”€â”€ processamento.py==
        ==â””â”€â”€ tratamento.py==

[^1]: 
